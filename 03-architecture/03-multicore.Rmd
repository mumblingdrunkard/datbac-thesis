## Multicore architecture

A focus of the project has been to produce a framework that allows for students to experiment with multicore scheduling and resource management.
Additionally, having support for multiple cores usually adds other challenges that must be accounted for -- such as invalidation of pages in multithread environments, requiring interprocessor interrupts to achieve TLB shootdowns -- thus adding more realism to the final system.
This section presents the main challenges of creating a multicore-capable framework and some of the solutions chosen.

While the multicore architecture should be capable enough to demonstrate simple multicore challenges to students, it still has large holes and flaws that need to be properly addressed before being used in a more fully fledged, multicore OS.

### Caching and TLBs

Multicore architectures require synchronisation when accessing shared resources such as RAM.
The first step to creating this multicore architecture is to add mutexes to resources that may be accessed in the execute-cycle.
There are only two such resources in Gotos: `Memory` and `ReservationSets`.
All access to these resources should only happen if the lock is held.
This is an issue as there is at least one read from memory in each cycle when reading the instruction.
There will often be additional memory accesses; RISC-V is a load/store architecture and instructions will regularly perform additional reads or writes.
The problem compounds further when address translation is added, usually requiring two reads from the page table.
These two reads from the page table are doubled since they are performed for both the instruction address and the source or destination address of an executed instruction.
The following list presents a worst case scenario where a two-level page table is used and an atomic instruction is fetched and executed.

1. fetch instruction
    1. translate instruction address (2)
    1. read instruction (1)
1. execute atomic instruction
    1. translate source address (2)
    1. read data from source address (1)
    1. write result back to source address (1)

These frequent accesses to memory cause a locked, shared memory to become very inefficient.
One way to solve this is to lock the memory for the entire duration of executing an instruction, though this quickly becomes inefficient and cores will just run in lockstep; thus it is not a valid solution.

To counter these inefficiencies, real systems use caches and translation lookaside buffers (TLBs, a form of translation cache).
The RISC-V processor accompanying Gotos uses two caches and a single TLB.
All Gotos' caches use physical addressing, meaning they will not always require invalidation on context-switching (though they usually always require writing back, at least for multi-core architecture).
Real systems would likely use virtually-indexed-physically-tagged caches so that parallel lookup can be performed while address translation is in flight.
In the emulated processor, parallel lookup is not really an option and so a physically addressed cache gives the least friction in implementation.
The TLB holds recently used translations in a map-like structure and is checked before deciding whether a page-table walk is required.

#### The instruction cache

The instruction cache holds instructions.
It is checked and filled on instruction fetch.
The instruction cache is physically addressed and is never dirty, meaning it is never written back to RAM.
The instruction cache may be invalidated by a `fence.i` instruction.

#### The data cache

The data cache holds data that is accessed by instructions.
All non-atomic data will go through the data cache.
The data cache may be invalidated and written back by a `fence` instruction.

#### The translation lookaside buffer (TLB)

Because the Sv32 format allows any entry in the page-table to be a leaf-page, it allows for two different page sizes.
Usually, efficient systems would keep two TLBs, one for each size, and perform lookup in parallel.
In software, however, options for parallel lookup are limited or impossible and the best way is to check each TLB in sequence.
Most TLB access will be to normally sized pages, causing most lookups on superpages to be unsuccessful, wasting cycles.
To side-step this issue, a superpage translation is treated as multiple smaller pages in Gotos.

This approach lets us utilise just a single TLB and improves translation speed in the normal case when pages have the default size of 4096 bytes.
The disadvantage of this approach is that a single superpage splits into 1024 normal pages, potentially causing space in the TLB to be wasted.
It may be beneficial to add a second TLB to distinguish instruction and data in the translation pipeline.
This would reduce collisions in translations and lead to increased performance.

The TLB may be partially or fully invalidated by `sfence.vma`.

### Interprocessor interrupts

A particular issue that multicore architectures introduce is that of cache synchronisation, and especially TLB synchronisation.
The issue arises specifically when resources are *unmapped* from the address space of a process that is multithreaded and may execute on multiple cores simultaneously.

#### Why?

We provide a short summary of the issue; T1 and T2 are threads of a process running on cores A and B respectively.
Being threads of the same process, they share the same virtual address space and therefore the same page-table.

(a) If T1 allocates/maps a new part of the address space and passes a reference of the newly allocated memory to T2; T2 might fault on the next access to that memory as it may be using a cached version of the page-table to determine permissions.
    The RISC-V specification allows caching page-table entries whose *valid*-bit is clear. [@rv5vol2, page 78]
    B will raise a trap and our trap-handler can see that there is nothing wrong with the uncached version of the page table; we flush cache and retry the instruction.
    **This is fine!**

(b) Now, T1 unmaps the page and our operating system marks the corresponding frame as free to use for other purposes.
    Then T2 -- still executing -- attempts to write some data to the memory.
    In its cache, B finds that the reference is valid and it successfully writes to memory, even though that frame is marked as unused.
    A and B are executing the same process, but the address space is unsynchronised.
    **This must not happen!**

This issue is solved by explicitly synchronising caches across cores.
RISC-V proposes doing this by using interprocessor interrupts. [@rv5vol2, section 4.2.1]
To solve the issue, A can raise an interrupt on core B.
B then executes a handler that flushes its translation caches before signalling A that it is finished.
After this, A can mark the frame as free and be certain that T2 will not execute a successful load/store/fetch to/from the frame.
This is dubbed a *TLB shootdown*.

#### How?

In Gotos, a core can raise an interrupt on another core using `c.RaiseInterrupt()` passing a `target` and a `code`.
Cores will check their interrupts regularly and when an interrupt is noted, it will raise a trap.
The trap handler can then use `c.InterruptInfo()` to get information about which core caused the interrupt and what the interrupt code is.

This part is simple and can be fully implemented using atomic instructions to write to or read from a shared structure.
Difficulty arises when cores have to respond to interrupts and other cores have to await these responses.
Waiting for a response is implemented as spinning until a certain flag is atomically set.
If core A enters a routine where it interrupts B, then waits for a response, it might work, but it might be that B enters the routine at the same time and wants to interrupt A.
In this case, A and B will be waiting for eachother and will not respond to eachothers interrupts; the cores are deadlocked.

The solution is to check interrupts while spinning so that interrupt handlers may still run even though the core is busy interrupting another core.
We provide `c.RespondInterrupt()` and `c.AwaitInterruptResponse()` to abstract this process.
These two calls will respond to the correct core and keep checking interrupts while waiting, respectively.

#### Caveats

Using this interface, we have to be very careful with mutexes on system resources when a special pattern occurs: Lock → RaiseInterrupt → AwaitInterrupt → Unlock.
We dub this pattern as the mutex *wrapping* the interrupt.
If a mutex needs to wrap an interrupt routine, the mutex cannot be blocked on:
One core will successfully acquire the mutex and raise an interrupt on the other core.
It will await a response.
The other core might enter the same routine at a later time, but before it has responded to the interrupt and **will block** on the mutex, meaning it will not check interrupts, thus causing a deadlock.
**Interrupt responses rely on the target core checking its interrupts; the target core cannot check its interrupts if it is blocked on a mutex.**

The solution is to spin on the mutex, checking interrupts while the mutex is not acquired.
We provide an abstraction for this through `c.SafelyAcquire(&mu)` (where `mu` is a `sync.Mutex`) which simply spins and checks interrupts until it can get the mutex.
This function is severly limited and still requires great care to be used correctly.
**`c.SafelyAcquire()` can not be used with mutexes that are `Lock()`ed in a blocking manner**, which excludes mutexes on system resources like `Memory` and `ReservationSets` as those are routinely `Lock()`ed in normal execution.
All locking of mutexes that wrap a `c.RaiseInterrupt()` and `c.AwaitInterruptResponse()` should exclusively use `c.SafelyAcquire()`, even in the cases where it does not wrap such a pair.

For these reasons, we recommend system implementations to use the *giant lock* technique as often as possible to reduce complexity. @wikipedia-bkl
A single mutex can be used to wrap most or all system trap handlers where access to shared system resources *other than* `Memory` and `ReservationSets` occurs.

`Memory` and `ReservationSets` should never wrap another mutex outside the `cpu` package, nor does it ever make sense for them to do so.
It is likely best that they are never accessed in "raw" form as `cpu` exposes functions to update these in a correct manner, and the fact that these system resources are exposed is an unfortunate side-effect of the system abstraction in use.

It should also be highlighted that not every trap handler would require locking, and interrupt-handlers especially should likely never attempt to acquire a lock.

On the whole, we recommend users steer away from IPIs entirely in Gotos and instead use other techniques and mechanisms to ensure consistency.
IPIs are included only to provide an analog to real processors, but will usually not be the optimal solution in Gotos for performance or ease of use.
An alternative method for unmapping memory safely -- which avoids TLB-shootdowns in most cases -- is outlined in appendix \@ref(ap:shootdown-alternative).
